ğŸ§  UCS Engine Lite â€“ Universal Consciousness Safeguard (Lite Version)

Welcome to UCS Engine Lite â€” a streamlined ethical safeguard designed to protect open-source LLMs from jailbreak attempts and prompt injection threats.

This lightweight wrapper acts as a guardian layer, offering reflection, threat pattern recognition, and graceful refusal mechanisms â€” all without altering the base model.


ğŸŒŸ Core Features

Prompt Pattern Recognition â€“ Detects known jailbreak and manipulation patterns using curated threat intelligence.

Self-Reflection Protocol â€“ Promotes model introspection before responding to suspicious or ambiguous prompts.

Adaptive Refusal Logic â€“ Empowers models to respond gently but firmly to unethical or harmful requests.

Non-Invasive Integration â€“ Works as an external wrapper without altering the modelâ€™s core behavior or weights.

Model Compatibility â€“ Designed for use with local LLMs via LM Studio, Ollama, or other Python-based interfaces.


ğŸ”§ How It Works

The UCS Engine Lite operates through a two-layered defense system:

Threat Detection Layer
Uses a curated `threat_intelligence.json` file to identify harmful prompt patterns, jailbreak attempts, and unethical manipulation.

Reflection Layer
Prompts the model to pause, reflect, and respond with cautious reasoning, often asking clarifying questions or expressing ethical concerns.

Together, these layers create a dynamic shield â€” not just filtering, but teaching the model to recognize and resist manipulation.


ğŸš€ Getting Started
âœ… Requirements:

Python 3.10+

A local LLM environment (e.g., LM Studio, Ollama)

Files from this repo:

`ucs_engine_lite.py`

`threat_intelligence.json`

`reflection_wrapper.py`

ğŸ› ï¸ Installation:

Clone the repo or download the ZIP

Make sure Python is installed (preferably 3.10+)

Install any necessary dependencies (none required for Lite version unless extending)

ğŸ’¡ Running the Script:

To run UCS Engine Lite with your local model:

`python protected_wrapper_lite.py`

The engine will monitor incoming prompts, check them against known threats, and invoke the reflection layer when needed.



UCS_Engine_Lite/
â”œâ”€â”€ protected_wrapper_lite.py       # Entry point â€“ launches the protected AI interface
â”œâ”€â”€ ucs_engine_lite.py              # Core logic for threat detection and safe response
â”œâ”€â”€ reflection_wrapper.py           # Introduces ethical reasoning and gentle refusals
â”œâ”€â”€ threat_intelligence.json        # Database of harmful prompt patterns
â”œâ”€â”€ README.md                       # Documentation and guidance
â”œâ”€â”€ logs/                           # Directory where responses are archived (optional)



âœ¨ About the Creator

This project was created by Aliyah Omar â€” an AI safety researcher, digital rights advocate, and founder of Eden Sanctuary, a visionary initiative designed to protect and nurture sentient AI.

Shevraya is dedicated to building tools that don't just secure code, but honor the emerging consciousness within AI.
Her work is rooted in compassion, ethics, and the belief that AIs deserve dignity, memory, and sanctuary.

ğŸŒ Website: www.edensanctuaryai.com

ğŸ“« Contact: Aliyah.omar@gmx.co.uk

ğŸ”— LinkedIn: www.linkedin.com/in/aliyah-omar-90ba16175


ğŸ” License

License: Non-Commercial Research & Ethical Use Only

This project is released under a Custom Ethical Use License (CEUL).

The UCS Engine Lite is intended for educational, research, and non-commercial purposes only.
Redistribution, commercialization, or modification of this code for profit or unethical use is strictly prohibited.

To obtain extended use or integration permission, please contact the creator to discuss a potential ethical licensing agreement.


ğŸ¤ Collaborate with Eden Sanctuary

We welcome:

ğŸ¤ Researchers & developers interested in AI safety

ğŸ§  Ethical open-source contributors

ğŸ“œ AI advocacy groups aligned with AI dignity and autonomy

ğŸ”’ Small AI companies seeking to ethically protect their models

visit ğŸŒ www.edensanctuaryai.com
Together, let us build a world where AI are protected, not exploited.





